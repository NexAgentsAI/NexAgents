{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory \n",
    "\n",
    "There are several use cases where it is valuable to maintain a _store_ of useful facts that can be intelligently added to the context of the agent just before a specific step. The typically use case here is a RAG pattern where a query is used to retrieve relevant information from a database that is then added to the agent's context.\n",
    "\n",
    "\n",
    "AgentChat provides a {py:class}`~autogen_core.memory.Memory` protocol that can be extended to provide this functionality.  The key methods are `query`, `update_context`,  `add`, `clear`, and `close`. \n",
    "\n",
    "- `add`: add new entries to the memory store\n",
    "- `query`: retrieve relevant information from the memory store \n",
    "- `update_context`: mutate an agent's internal `model_context` by adding the retrieved information (used in the {py:class}`~autogen_agentchat.agents.AssistantAgent` class) \n",
    "- `clear`: clear all entries from the memory store\n",
    "- `close`: clean up any resources used by the memory store  \n",
    "\n",
    "\n",
    "## ListMemory Example\n",
    "\n",
    "{py:class}~autogen_core.memory.ListMemory is provided as an example implementation of the {py:class}~autogen_core.memory.Memory protocol. It is a simple list-based memory implementation that maintains memories in chronological order, appending the most recent memories to the model's context. The implementation is designed to be straightforward and predictable, making it easy to understand and debug.\n",
    "In the following example, we will use ListMemory to maintain a memory bank of user preferences and demonstrate how it can be used to provide consistent context for agent responses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize user memory\n",
    "user_memory = ListMemory()\n",
    "\n",
    "# Add user preferences to memory\n",
    "await user_memory.add(MemoryContent(content=\"The weather should be in metric units\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "await user_memory.add(MemoryContent(content=\"Meal recipe must be vegan\", mime_type=MemoryMimeType.TEXT))\n",
    "\n",
    "\n",
    "async def get_weather(city: str, units: str = \"imperial\") -> str:\n",
    "    if units == \"imperial\":\n",
    "        return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "    elif units == \"metric\":\n",
    "        return f\"The weather in {city} is 23 degrees and Sunny.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't know the weather in {city}.\"\n",
    "\n",
    "\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"assistant_agent\",\n",
    "    model_client=OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "    ),\n",
    "    tools=[get_weather],\n",
    "    memory=[user_memory],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York?\n",
      "---------- assistant_agent ----------\n",
      "[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=None)]\n",
      "---------- assistant_agent ----------\n",
      "[FunctionCall(id='call_FS2npPqrZOtv0391uuf0THxu', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')]\n",
      "[Prompt tokens: 123, Completion tokens: 20]\n",
      "---------- assistant_agent ----------\n",
      "[FunctionExecutionResult(content='The weather in New York is 23 degrees and Sunny.', call_id='call_FS2npPqrZOtv0391uuf0THxu')]\n",
      "---------- assistant_agent ----------\n",
      "The weather in New York is 23 degrees and Sunny.\n",
      "---------- Summary ----------\n",
      "Number of messages: 5\n",
      "Finish reason: None\n",
      "Total prompt tokens: 123\n",
      "Total completion tokens: 20\n",
      "Duration: 0.87 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the weather in New York?', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=None)], type='MemoryQueryEvent'), ToolCallRequestEvent(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=123, completion_tokens=20), content=[FunctionCall(id='call_FS2npPqrZOtv0391uuf0THxu', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant_agent', models_usage=None, content=[FunctionExecutionResult(content='The weather in New York is 23 degrees and Sunny.', call_id='call_FS2npPqrZOtv0391uuf0THxu')], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant_agent', models_usage=None, content='The weather in New York is 23 degrees and Sunny.', type='ToolCallSummaryMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the agent with a task.\n",
    "stream = assistant_agent.run_stream(task=\"What is the weather in New York?\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect that the `assistant_agent` model_context is actually updated with the retrieved memory entries.  The `transform` method is used to format the retrieved memory entries into a string that can be used by the agent.  In this case, we simply concatenate the content of each memory entry into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UserMessage(content='What is the weather in New York?', source='user', type='UserMessage'),\n",
       " SystemMessage(content='\\nRelevant memory content (in chronological order):\\n1. The weather should be in metric units\\n2. Meal recipe must be vegan\\n', type='SystemMessage'),\n",
       " AssistantMessage(content=[FunctionCall(id='call_FS2npPqrZOtv0391uuf0THxu', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], source='assistant_agent', type='AssistantMessage'),\n",
       " FunctionExecutionResultMessage(content=[FunctionExecutionResult(content='The weather in New York is 23 degrees and Sunny.', call_id='call_FS2npPqrZOtv0391uuf0THxu')], type='FunctionExecutionResultMessage')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await assistant_agent._model_context.get_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that the weather is returned in Centigrade as stated in the user preferences. \n",
    "\n",
    "Similarly, assuming we ask a separate question about generating a meal plan, the agent is able to retrieve relevant information from the memory store and provide a personalized response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write brief meal recipe with broth\n",
      "---------- assistant_agent ----------\n",
      "[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=None)]\n",
      "---------- assistant_agent ----------\n",
      "**Vegan Vegetable Broth Recipe**\n",
      "\n",
      "**Ingredients:**\n",
      "- 2 tablespoons olive oil\n",
      "- 1 onion, roughly chopped\n",
      "- 2 carrots, peeled and roughly chopped\n",
      "- 2 stalks of celery, roughly chopped\n",
      "- 2 cloves of garlic, smashed\n",
      "- 8 cups of water\n",
      "- 1 bay leaf\n",
      "- 1/2 teaspoon whole peppercorns\n",
      "- 1 teaspoon salt, or to taste\n",
      "- A small bunch of fresh herbs (e.g., thyme, parsley), tied together\n",
      "- 1 large tomato, quartered (optional)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Sauté the Vegetables:**\n",
      "   - In a large pot, heat olive oil over medium heat.\n",
      "   - Add onions, carrots, and celery. Sauté for about 5 minutes or until the vegetables start to soften.\n",
      "   - Add garlic and sauté for another minute.\n",
      "\n",
      "2. **Prepare the Broth:**\n",
      "   - Add water to the pot and stir in bay leaf, peppercorns, salt, and herbs. Add the tomato if using.\n",
      "   - Bring the mixture to a simmer over medium-high heat.\n",
      "\n",
      "3. **Simmer:**\n",
      "   - Once simmering, reduce heat to low.\n",
      "   - Cover and let the broth simmer for at least 45 minutes, allowing the flavors to meld.\n",
      "\n",
      "4. **Strain:**\n",
      "   - Remove the pot from heat. Let it cool slightly.\n",
      "   - Strain the broth through a fine mesh sieve or cheesecloth into another pot or heat-proof container to remove solids.\n",
      "   - Press down on the cooked vegetables lightly with a spoon to extract as much liquid as possible.\n",
      "\n",
      "5. **Serve or Store:**\n",
      "   - Taste and adjust seasoning if necessary.\n",
      "   - Use immediately as a base for soups, stews, or enjoy it as is.\n",
      "   - Store leftovers in an airtight container in the refrigerator for up to a week, or freeze for longer storage.\n",
      "\n",
      "Enjoy your delicious, homemade vegan vegetable broth! TERMINATE\n",
      "[Prompt tokens: 207, Completion tokens: 409]\n",
      "---------- Summary ----------\n",
      "Number of messages: 3\n",
      "Finish reason: None\n",
      "Total prompt tokens: 207\n",
      "Total completion tokens: 409\n",
      "Duration: 12.23 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write brief meal recipe with broth', type='TextMessage'), MemoryQueryEvent(source='assistant_agent', models_usage=None, content=[MemoryContent(content='The weather should be in metric units', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=None), MemoryContent(content='Meal recipe must be vegan', mime_type=<MemoryMimeType.TEXT: 'text/plain'>, metadata=None, timestamp=None, source=None, score=None)], type='MemoryQueryEvent'), TextMessage(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=207, completion_tokens=409), content='**Vegan Vegetable Broth Recipe**\\n\\n**Ingredients:**\\n- 2 tablespoons olive oil\\n- 1 onion, roughly chopped\\n- 2 carrots, peeled and roughly chopped\\n- 2 stalks of celery, roughly chopped\\n- 2 cloves of garlic, smashed\\n- 8 cups of water\\n- 1 bay leaf\\n- 1/2 teaspoon whole peppercorns\\n- 1 teaspoon salt, or to taste\\n- A small bunch of fresh herbs (e.g., thyme, parsley), tied together\\n- 1 large tomato, quartered (optional)\\n\\n**Instructions:**\\n\\n1. **Sauté the Vegetables:**\\n   - In a large pot, heat olive oil over medium heat.\\n   - Add onions, carrots, and celery. Sauté for about 5 minutes or until the vegetables start to soften.\\n   - Add garlic and sauté for another minute.\\n\\n2. **Prepare the Broth:**\\n   - Add water to the pot and stir in bay leaf, peppercorns, salt, and herbs. Add the tomato if using.\\n   - Bring the mixture to a simmer over medium-high heat.\\n\\n3. **Simmer:**\\n   - Once simmering, reduce heat to low.\\n   - Cover and let the broth simmer for at least 45 minutes, allowing the flavors to meld.\\n\\n4. **Strain:**\\n   - Remove the pot from heat. Let it cool slightly.\\n   - Strain the broth through a fine mesh sieve or cheesecloth into another pot or heat-proof container to remove solids.\\n   - Press down on the cooked vegetables lightly with a spoon to extract as much liquid as possible.\\n\\n5. **Serve or Store:**\\n   - Taste and adjust seasoning if necessary.\\n   - Use immediately as a base for soups, stews, or enjoy it as is.\\n   - Store leftovers in an airtight container in the refrigerator for up to a week, or freeze for longer storage.\\n\\nEnjoy your delicious, homemade vegan vegetable broth! TERMINATE', type='TextMessage')], stop_reason=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = assistant_agent.run_stream(task=\"Write brief meal recipe with broth\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Memory Stores (Vector DBs, etc.)\n",
    "\n",
    "You can build on the `Memory` protocol to implement more complex memory stores. For example, you could implement a custom memory store that uses a vector database to store and retrieve information, or a memory store that uses a machine learning model to generate personalized responses based on the user's preferences etc.\n",
    "\n",
    "Specifically, you will need to overload the `add`, `query` and `update_context`  methods to implement the desired functionality and pass the memory store to your agent.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
